{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vappiah/Bioinformatics/blob/master/notebooks/evaluation/Compare_Classifiers_using_Heatmaps_Episode2_AUC_Scores.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b05c990e",
      "metadata": {
        "id": "b05c990e"
      },
      "source": [
        "# Compare Classifiers using Heatmaps-Episode-2 \n",
        "In this tutorial we are going to compute AUC scores for all classes per model and use the result to generate a heatmap"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecf55a26",
      "metadata": {
        "id": "ecf55a26"
      },
      "source": [
        "### Classifiers\n",
        " - Adaboost\n",
        " - Extra Trees\n",
        " - K-Nearest Neighbor\n",
        " - Logistic Regression\n",
        " - Naive Bayes\n",
        " - Random Forests\n",
        " - Support Vector Machines\n",
        " - XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d7202fe",
      "metadata": {
        "id": "5d7202fe"
      },
      "source": [
        "## Required Libraries\n",
        " - numpy\n",
        " - matplotlib\n",
        " - seaborn\n",
        " - pandas\n",
        " - scikit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91f3dde3",
      "metadata": {
        "id": "91f3dde3"
      },
      "source": [
        "## Import Python libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0515dec",
      "metadata": {
        "id": "a0515dec"
      },
      "outputs": [],
      "source": [
        "#data handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "#classification\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# performance metrics\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b45976a3",
      "metadata": {
        "id": "b45976a3"
      },
      "source": [
        "## Read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25a5cb29",
      "metadata": {
        "id": "25a5cb29"
      },
      "outputs": [],
      "source": [
        "\n",
        "#read data directly from a github repository\n",
        "\n",
        "file_url='https://github.com/vappiah/Machine-Learning-Tutorials/raw/main/data/cancer_gene_expression.zip'\n",
        "\n",
        "dataframe=pd.read_csv(file_url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a0e165b",
      "metadata": {
        "id": "7a0e165b"
      },
      "source": [
        " \n",
        "## **Data preprocesing** \n",
        "This is done to put the data in an appropriate format before modelling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a13b9f51",
      "metadata": {
        "id": "a13b9f51"
      },
      "outputs": [],
      "source": [
        "#we will now seperate the feature values from the class. we do this because scikit-learn requires that features and class are separated before parsing them to the classifiers.\n",
        "\n",
        "X=dataframe.iloc[:,0:-1]\n",
        "y=dataframe.iloc[:,-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "264a01cb",
      "metadata": {
        "id": "264a01cb"
      },
      "source": [
        "\\\n",
        "**Encode labels**\n",
        "\n",
        "The labels for this data are categorical and we therefore have to convert them to numeric forms. This is referred to as encoding. Machine learning models usually require input data to be in numeric forms, hence we encoding the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b251f08b",
      "metadata": {
        "id": "b251f08b"
      },
      "outputs": [],
      "source": [
        "#let's encode target labels (y) with values between 0 and n_classes-1.\n",
        "#encoding will be done using the LabelEncoder\n",
        "label_encoder=LabelEncoder()\n",
        "label_encoder.fit(y)\n",
        "y_encoded=label_encoder.transform(y)\n",
        "labels=label_encoder.classes_\n",
        "classes=np.unique(y_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f68c4f6",
      "metadata": {
        "id": "5f68c4f6"
      },
      "source": [
        "\\\n",
        "**Data Splitting**\\\n",
        "We will now split the data into training and test subsets.\n",
        "The training data is initially parsed to the machine learning model. this is to enable the model to identify discriminatory patterns which can be used to make future predictions.\n",
        "The testing data is used to evaluate the model after the training phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fada994",
      "metadata": {
        "id": "6fada994"
      },
      "outputs": [],
      "source": [
        "#split data into training and test sets\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y_encoded,test_size=0.2,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d1c8d5e",
      "metadata": {
        "id": "0d1c8d5e"
      },
      "source": [
        "\\\n",
        "**Data Normalization**\\\n",
        "Data normalization is done so that the values are in the same range. This will improve model performance and avoid bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "909e93b4",
      "metadata": {
        "id": "909e93b4"
      },
      "outputs": [],
      "source": [
        "# scale data between 0 and 1\n",
        "\n",
        "min_max_scaler=MinMaxScaler()\n",
        "X_train_norm=min_max_scaler.fit_transform(X_train)\n",
        "X_test_norm=min_max_scaler.fit_transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a1736a7",
      "metadata": {
        "id": "7a1736a7"
      },
      "source": [
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2127f034",
      "metadata": {
        "id": "2127f034"
      },
      "source": [
        "## **Feature Selection**\n",
        "The purpose of feature selection is to select relevant features for classification. \n",
        "Feature selection is usually used as a pre-processing step before doing the actual learning. \n",
        "\n",
        "In this tutorial, mutual information algorithm is used to compute the relevance of each feature. The top n (eg. 300) features are selected for the machine learning analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4665aa65",
      "metadata": {
        "id": "4665aa65"
      },
      "source": [
        "### Feature Selection using Mutual Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe32fd84",
      "metadata": {
        "id": "fe32fd84"
      },
      "outputs": [],
      "source": [
        "MI=mutual_info_classif(X_train_norm,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "697d3a17",
      "metadata": {
        "id": "697d3a17"
      },
      "outputs": [],
      "source": [
        "#select top n features. lets say 300.\n",
        "#you can modify the value and see how the performance of the model changes\n",
        "\n",
        "n_features=300\n",
        "selected_scores_indices=np.argsort(MI)[::-1][0:n_features]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45740745",
      "metadata": {
        "id": "45740745"
      },
      "outputs": [],
      "source": [
        "X_train_selected=X_train_norm[:,selected_scores_indices]\n",
        "X_test_selected=X_test_norm[:,selected_scores_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58c6d53d",
      "metadata": {
        "id": "58c6d53d"
      },
      "source": [
        "## Classification\n",
        "The random forest classifier is used in this tutorial. Random forest works with multiclass and high dimensional data. Classification will involve training and testing of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0f0ba2c",
      "metadata": {
        "id": "c0f0ba2c"
      },
      "source": [
        "### Model Training\n",
        "Training allows the machine learning model to learn from the data and use the identified patterns to predict the outcomes of data it has never seen before.\n",
        "In the training phase, the model is given the training subset. In this tutorial, the Random Forest Classifier is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25201362",
      "metadata": {
        "id": "25201362"
      },
      "outputs": [],
      "source": [
        "def compute_auc(model,xtest,ytest):\n",
        "    \n",
        "    if hasattr(model,'decision_function'):\n",
        "        probs=model.decision_function(xtest) \n",
        "    elif hasattr(model,'predict_proba'):\n",
        "        #returns the positive outcomes\n",
        "        probs=model.predict_proba(xtest)\n",
        "    \n",
        "    \n",
        "\n",
        "    y_test_binarized=label_binarize(y_test,classes=classes)\n",
        "\n",
        "    # roc curve for classes\n",
        "    fpr = {}\n",
        "    tpr = {}\n",
        "    thresh ={}\n",
        "    roc_auc_dict = dict()\n",
        "    n_class = classes.shape[0]\n",
        "    for i in range(n_class):    \n",
        "        roc_auc_dict[labels[i]] = roc_auc_score(y_test_binarized[:,i], probs[:,i],multi_class='ovr',\n",
        "                                                average='weighted')\n",
        "    \n",
        "    return roc_auc_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6051fea6",
      "metadata": {
        "id": "6051fea6"
      },
      "outputs": [],
      "source": [
        "def fit_data(xtrain,ytrain,xtest,ytest):\n",
        "    \n",
        "    #Adaboost Classifier\n",
        "    ADB=AdaBoostClassifier()\n",
        "    ADB.fit(xtrain,ytrain)\n",
        "\n",
        "    #XGBoost Classifier\n",
        "    XGB=XGBClassifier(num_class=labels.shape,eval_metric='mlogloss',use_label_encoder =False)\n",
        "    XGB.fit(xtrain,ytrain)\n",
        "    \n",
        "    #Random Forest Classifier\n",
        "    RF=RandomForestClassifier(max_features=0.2)\n",
        "    RF.fit(xtrain,ytrain)\n",
        "\n",
        "    #Support Vector Machine Classifier\n",
        "    SVM=SVC()\n",
        "    SVM.fit(xtrain,ytrain)\n",
        "\n",
        "    #K-nearest Neighbor classifier\n",
        "    KNN=KNeighborsClassifier()\n",
        "    KNN.fit(xtrain,ytrain)\n",
        "\n",
        "    #Naive Bayes Classifier\n",
        "    NB=GaussianNB()\n",
        "    NB.fit(xtrain,ytrain)\n",
        "\n",
        "    #Extra Trees Classifier\n",
        "    ETC=ExtraTreesClassifier()\n",
        "    ETC.fit(xtrain,ytrain)\n",
        "\n",
        "    #Logistic Regression\n",
        "    \n",
        "    LOGREG =LogisticRegression(C = 50, multi_class = 'multinomial',solver='lbfgs', max_iter=3000)\n",
        "    LOGREG.fit(xtrain,ytrain)\n",
        "\n",
        "\t\n",
        "    #create a dictionary object to store the models\n",
        "    models={'KNN':KNN,'Random Forest':RF,'LOGREG':LOGREG,'SVM':SVM,'Naive Bayes':NB,\n",
        "            'XGBoost':XGB,'Extra Trees':ETC,'Adaboost':ADB}\n",
        "    \n",
        "    return models\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "Compare Classifiers using Heatmaps-Episode2-AUC Scores.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}